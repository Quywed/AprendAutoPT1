{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa168ab-4500-4d22-aba9-989df3e3274d",
   "metadata": {},
   "source": [
    "# 1. Import de Bibliotecas e de Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c076668-f3b0-4c9b-849a-b4d4235502b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Importar modelos\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f614e-fa19-4055-ab8b-e4ed7ffe1037",
   "metadata": {},
   "source": [
    "# 2. FEATURE ENGINEERING - Extracao e Limpeza de Dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7755fa7-15bb-4e13-9d7e-9c5da12eae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engeneering(df):\n",
    "    df_eng = df.copy()\n",
    "\n",
    "    # --- LIMPEZA E EXTRA√á√ÉO ---\n",
    "    # Extrair n√∫meros\n",
    "    df_eng['hp'] = df['engine'].str.extract(r'(\\d+\\.?\\d*)HP', expand=False).astype(float)\n",
    "    df_eng['liters'] = df['engine'].str.extract(r'(\\d+\\.?\\d*)L\\s', expand=False).astype(float)\n",
    "    \n",
    "    # IMPORTANTE: Preencher Nulos com a Mediana da Coluna (em vez de 0)\n",
    "    # 0 HP ou 0 Litros destr√≥i a l√≥gica do modelo\n",
    "    df_eng['hp'] = df_eng['hp'].fillna(df_eng['hp'].median())\n",
    "    df_eng['liters'] = df_eng['liters'].fillna(df_eng['liters'].median())\n",
    "\n",
    "    # --- Idade ---\n",
    "    var_ano_atual = date.today().year\n",
    "    df_eng['car_age'] = var_ano_atual - df_eng['model_year']\n",
    "    \n",
    "    # --- Features de Texto (Simples) ---\n",
    "    df_eng['is_turbo'] = df['engine'].str.contains(r'(?i)turbo', na=False).astype(int)\n",
    "    \n",
    "    # Transmission Mapping Simples\n",
    "    def clean_transmission(val):\n",
    "        s = str(val).lower()\n",
    "        if 'automatic' in s or 'a/t' in s or 'cvt' in s: return 'Automatico'\n",
    "        if 'manual' in s or 'm/t' in s: return 'Manual'\n",
    "        return 'Outro'\n",
    "    df_eng['transmission_type'] = df_eng['transmission'].apply(clean_transmission)\n",
    "\n",
    "    # Fuel Mapping Simples\n",
    "    def clean_fuel(val):\n",
    "        s = str(val).lower()\n",
    "        if 'hybrid' in s: return 'Hybrid'\n",
    "        if 'electric' in s: return 'EV'\n",
    "        return 'Gasoline' # Simplificar ao m√°ximo para evitar categorias raras\n",
    "    df_eng['fuel_type'] = df_eng['fuel_type'].apply(clean_fuel)\n",
    "\n",
    "    return df_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d4bec-25ba-4934-94ae-5b1b7fde8338",
   "metadata": {},
   "source": [
    "# 3. PREPARA√á√ÉO DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a271f79-8b4a-4e82-973c-5c6148d06d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_dados(df_treino, df_teste):\n",
    "    # 1. Feature Engineering Base\n",
    "    df_treino_eng = feature_engeneering(df_treino)\n",
    "    df_teste_eng = feature_engeneering(df_teste)\n",
    "\n",
    "    # 2. Separar Target e aplicar LOG (Fundamental para pre√ßos)\n",
    "    y = np.log1p(df_treino_eng['price']) # Treinar sempre no LOG do pre√ßo\n",
    "    X = df_treino_eng.drop(['price'], axis=1)\n",
    "    X_test = df_teste_eng.copy()\n",
    "\n",
    "    # 3. Target Encoding Manual (Crucial!)\n",
    "    # Para cada coluna categ√≥rica, substitu√≠mos o texto pela m√©dia de pre√ßo desse texto no treino\n",
    "    cols_para_encode = ['brand', 'model', 'transmission_type', 'fuel_type', 'int_col', 'ext_col']\n",
    "    \n",
    "    # Dicion√°rio para guardar os mapas (para usar no teste depois)\n",
    "    mapas_encoding = {}\n",
    "\n",
    "    for col in cols_para_encode:\n",
    "        # Calcular a m√©dia do LOG price por categoria\n",
    "        # Usamos o X junto com o y temporariamente para calcular\n",
    "        temp_df = pd.concat([X, y], axis=1)\n",
    "        media_categoria = temp_df.groupby(col)['price'].mean()\n",
    "        \n",
    "        # Guardar mapa (se uma categoria nova aparecer no teste, usamos a m√©dia global)\n",
    "        media_global = y.mean()\n",
    "        \n",
    "        # Aplicar no Treino\n",
    "        X[col + '_target'] = X[col].map(media_categoria)\n",
    "        X[col + '_target'] = X[col + '_target'].fillna(media_global) # Preencher vazios\n",
    "        \n",
    "        # Aplicar no Teste (Usando as m√©dias do TREINO - para n√£o vazar dados)\n",
    "        X_test[col + '_target'] = X_test[col].map(media_categoria)\n",
    "        X_test[col + '_target'] = X_test[col + '_target'].fillna(media_global)\n",
    "\n",
    "    # 4. Selecionar apenas colunas num√©ricas para o modelo\n",
    "    cols_finais = ['model_year', 'hp', 'liters', 'car_age', 'is_turbo'] + [c + '_target' for c in cols_para_encode]\n",
    "    \n",
    "    return X[cols_finais], y, X_test[cols_finais], None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195a7cf-1576-4570-9e78-fb1a6cd84404",
   "metadata": {},
   "source": [
    "# 4. DEFINI√á√ÉO DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08236dd4-e05b-4131-bb66-cbf6ff847536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_modelos():\n",
    "    \"\"\"Retorna dicion√°rio com todos os modelos dispon√≠veis\"\"\"\n",
    "\n",
    "    modelos = {\n",
    "        # Regress√£o Linear\n",
    "        #'Linear Regression': LinearRegression(),\n",
    "\n",
    "        # K-Nearest Neighbors\n",
    "        #'#KNN': KNeighborsRegressor(n_neighbors=5),\n",
    "\n",
    "        # √Årvores de Decis√£o\n",
    "        #'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "\n",
    "        # Random Forest e variantes\n",
    "        #'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, \n",
    "                                               #random_state=42, n_jobs=-1),\n",
    "\n",
    "        'XGBoost': xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, \n",
    "                                    max_depth=7, random_state=42, n_jobs=-1),\n",
    "\n",
    "        # Support Vector Machines\n",
    "        #'SVR Linear': SVR(kernel='linear', C=1.0),\n",
    "        #SVR RBF': SVR(kernel='rbf', C=1.0, gamma='scale'),\n",
    "\n",
    "        # Redes Neuronais\n",
    "        #'MLP Small': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, \n",
    "                                  #random_state=42, early_stopping=True),\n",
    "\n",
    "        #'MLP Deep': MLPRegressor(hidden_layer_sizes=(100, 50, 25), max_iter=500, \n",
    "                                 #random_state=42, early_stopping=True)\n",
    "    }\n",
    "\n",
    "    return modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102bd35-0181-458e-bd8c-174dcd8a59e9",
   "metadata": {},
   "source": [
    "# 5. GridSearchCV - Grid Search com Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b197afb0-58d0-42b8-8035-f1242807e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. GridSearchCV - Otimiza√ß√£o de Hiperpar√¢metros\n",
    "def obter_params_grid(nome_modelo):\n",
    "    \"\"\"\n",
    "    Retorna o dicion√°rio de hiperpar√¢metros para testar\n",
    "    baseado no nome do modelo.\n",
    "    \"\"\"\n",
    "    grids = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 500, 1000],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.7, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.9, 1.0]\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': [3, 5, 7, 11],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'p': [1, 2] # 1=Manhattan, 2=Euclidean\n",
    "        },\n",
    "        'MLP Deep': {\n",
    "            'hidden_layer_sizes': [(50,50,50), (100,50), (100,)],\n",
    "            'activation': ['tanh', 'relu'],\n",
    "            'alpha': [0.0001, 0.05],\n",
    "        },\n",
    "        'SVR RBF': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "            'epsilon': [0.1, 0.2, 0.5]\n",
    "        }\n",
    "    }\n",
    "    return grids.get(nome_modelo, {})\n",
    "\n",
    "def executar_grid_search(modelo, nome_modelo, X, y, usar_scaled=False):\n",
    "    \"\"\"\n",
    "    Executa o GridSearch para encontrar os melhores hiperpar√¢metros.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Iniciando GridSearch para: {nome_modelo}\")\n",
    "\n",
    "    # 1. Obter a grelha de par√¢metros\n",
    "    param_grid = obter_params_grid(nome_modelo)\n",
    "\n",
    "    if not param_grid:\n",
    "        print(f\"‚ö†Ô∏è Nenhuma grid definida para {nome_modelo}. Retornando modelo base.\")\n",
    "        return modelo\n",
    "\n",
    "    # 2. Configurar o GridSearch\n",
    "    # cv=3 ou 5 (Cross Validation)\n",
    "    # scoring='neg_root_mean_squared_error' para minimizar o RMSE\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=modelo,\n",
    "        param_grid=param_grid,\n",
    "        cv=3, \n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        verbose=1,\n",
    "        n_jobs=-1  # Usa todos os processadores\n",
    "    )\n",
    "\n",
    "    # 3. Treinar\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # 4. Resultados\n",
    "    print(f\"‚úÖ Melhores Par√¢metros: {grid_search.best_params_}\")\n",
    "    print(f\"   Melhor RMSE (CV): {-grid_search.best_score_:,.2f}\")\n",
    "    return grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092de71f-bf19-47ce-bdb7-3571e098690c",
   "metadata": {},
   "source": [
    "# 6. AVALIA√á√ÉO DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e5ca01-d83a-4a23-83d3-f467ea95343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_modelo(modelo, X_train, y_train, X_val, y_val, nome_modelo):\n",
    "    \"\"\"Treina e avalia um modelo\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Treinando: {nome_modelo}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Treinar\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    # Predi√ß√µes\n",
    "    y_train_pred = modelo.predict(X_train)\n",
    "    y_val_pred = modelo.predict(X_val)\n",
    "\n",
    "    # M√©tricas de treino\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "    # M√©tricas de valida√ß√£o\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "    # Exibir resultados\n",
    "    print(f\"\\nüìä M√âTRICAS DE TREINO:\")\n",
    "    print(f\"  RMSE: ${train_rmse:,.2f}\")\n",
    "    print(f\"  R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"  MAE: ${train_mae:,.2f}\")\n",
    "\n",
    "    print(f\"\\nüìä M√âTRICAS DE VALIDA√á√ÉO:\")\n",
    "    print(f\"  RMSE: ${val_rmse:,.2f}\")\n",
    "    print(f\"  R¬≤: {val_r2:.4f}\")\n",
    "    print(f\"  MAE: ${val_mae:,.2f}\")\n",
    "\n",
    "    # Verificar overfitting\n",
    "    overfit = train_r2 - val_r2\n",
    "    if overfit > 0.1:\n",
    "        print(f\"\\n‚ö†Ô∏è  Poss√≠vel overfitting detectado (diferen√ßa R¬≤: {overfit:.4f})\")\n",
    "\n",
    "    return {\n",
    "        'modelo': nome_modelo,\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'train_mae': train_mae,\n",
    "        'val_rmse': val_rmse,\n",
    "        'val_r2': val_r2,\n",
    "        'val_mae': val_mae,\n",
    "        'modelo_treinado': modelo\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ed31d-3e69-4843-a906-ab208a792bb3",
   "metadata": {},
   "source": [
    "# 7. TREINAR MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ac6f85-9dad-4fcd-8f5e-cbf989845ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelos(df_treino, df_teste, modelos_selecionados=None):\n",
    "    \"\"\"\n",
    "    Treina e compara m√∫ltiplos modelos\n",
    "\n",
    "    Args:\n",
    "        df_treino: DataFrame de treino\n",
    "        df_teste: DataFrame de teste\n",
    "        modelos_selecionados: Lista com nomes dos modelos a treinar (None = todos)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîÑ Preparando dados...\")\n",
    "    X, y, X_test, encoders = preparar_dados(df_treino, df_teste)\n",
    "\n",
    "    # Split treino/valida√ß√£o\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Normalizar dados (importante para KNN, SVM e MLP)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"‚úÖ Dados preparados:\")\n",
    "    print(f\"  - Treino: {X_train.shape[0]} amostras\")\n",
    "    print(f\"  - Valida√ß√£o: {X_val.shape[0]} amostras\")\n",
    "    print(f\"  - Features: {X_train.shape[1]}\")\n",
    "\n",
    "    # Obter modelos\n",
    "    todos_modelos = obter_modelos()\n",
    "\n",
    "    # Filtrar modelos se especificado\n",
    "    if modelos_selecionados:\n",
    "        modelos = {k: v for k, v in todos_modelos.items() if k in modelos_selecionados}\n",
    "    else:\n",
    "        modelos = todos_modelos\n",
    "\n",
    "    # Treinar modelos\n",
    "    resultados = []\n",
    "    modelos_treinados = {}\n",
    "\n",
    "    for nome, modelo in modelos.items():\n",
    "        # Decidir se usar dados normalizados\n",
    "        usar_scaled = nome in ['KNN', 'SVR Linear', 'SVR RBF', 'MLP Small', 'MLP Deep']\n",
    "\n",
    "        if usar_scaled:\n",
    "            res = avaliar_modelo(modelo, X_train_scaled, y_train, \n",
    "                                X_val_scaled, y_val, nome)\n",
    "        else:\n",
    "            res = avaliar_modelo(modelo, X_train, y_train, \n",
    "                                X_val, y_val, nome)\n",
    "\n",
    "        resultados.append(res)\n",
    "        modelos_treinados[nome] = {\n",
    "            'modelo': res['modelo_treinado'],\n",
    "            'usar_scaled': usar_scaled\n",
    "        }\n",
    "\n",
    "    # Compara√ß√£o final\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"üìà COMPARA√á√ÉO DE MODELOS (ordenados por R¬≤ de valida√ß√£o)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    df_resultados = df_resultados.sort_values('val_r2', ascending=False)\n",
    "\n",
    "    print(df_resultados[['modelo', 'val_rmse', 'val_r2', 'val_mae']].to_string(index=False))\n",
    "\n",
    "    # Melhor modelo\n",
    "    melhor = df_resultados.iloc[0]\n",
    "    print(f\"\\n\\nüèÜ MELHOR MODELO: {melhor['modelo']}\")\n",
    "    print(f\"  R¬≤ Valida√ß√£o: {melhor['val_r2']:.4f}\")\n",
    "    print(f\"  RMSE Valida√ß√£o: ${melhor['val_rmse']:,.2f}\")\n",
    "\n",
    "    return df_resultados, modelos_treinados, scaler, (X_test, X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2617259-8839-42b6-90ae-342a9f94ee44",
   "metadata": {},
   "source": [
    "# 8. Guardar Hiperparametros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44766b71-4b34-414b-9a42-57479e8fa907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_submissao_log(df_sub, modelo_treinado, nome_modelo, metricas):\n",
    "    \"\"\"\n",
    "    Salva CSV e JSON incrementando o ID com base no maior n√∫mero encontrado.\n",
    "    \"\"\"\n",
    "    pasta = 'submissoes'\n",
    "    os.makedirs(pasta, exist_ok=True)\n",
    "\n",
    "    # 1. Listar arquivos e encontrar o maior ID existente\n",
    "    arquivos = os.listdir(pasta)\n",
    "    ids_existentes = []\n",
    "\n",
    "    for f in arquivos:\n",
    "        # Verifica se o arquivo segue o padr√£o 'submission_X.csv'\n",
    "        if f.startswith('submission_') and f.endswith('.csv'):\n",
    "            try:\n",
    "                # Extrai apenas o n√∫mero do nome do arquivo\n",
    "                # Ex: 'submission_12.csv' -> '12'\n",
    "                numero_str = f.replace('submission_', '').replace('.csv', '')\n",
    "                ids_existentes.append(int(numero_str))\n",
    "            except ValueError:\n",
    "                continue # Pula arquivos que n√£o tenham n√∫mero v√°lido\n",
    "\n",
    "    # Se a lista estiver vazia, come√ßa do 1. Se n√£o, pega o maior + 1\n",
    "    if not ids_existentes:\n",
    "        next_id = 1\n",
    "    else:\n",
    "        next_id = max(ids_existentes) + 1\n",
    "\n",
    "    # 2. Definir nomes dos arquivos\n",
    "    filename_csv = f\"{pasta}/submission_{next_id}.csv\"\n",
    "    filename_json = f\"{pasta}/submission_{next_id}_params.json\"\n",
    "\n",
    "    # 3. Salvar CSV\n",
    "    df_sub.to_csv(filename_csv, index=False)\n",
    "\n",
    "    # 4. Extrair Hiperpar√¢metros\n",
    "    try:\n",
    "        params = modelo_treinado.get_params()\n",
    "    except:\n",
    "        params = {\"info\": \"N√£o foi poss√≠vel extrair params\"}\n",
    "\n",
    "    # 5. Metadata\n",
    "    metadata = {\n",
    "        \"id\": next_id,\n",
    "        \"modelo\": nome_modelo,\n",
    "        \"performance_validacao\": metricas,\n",
    "        \"hiperparametros\": params\n",
    "    }\n",
    "\n",
    "    # 6. Salvar JSON\n",
    "    with open(filename_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=4, default=str)\n",
    "\n",
    "    print(f\"\\n‚úÖ Submiss√£o #{next_id} salva com sucesso!\")\n",
    "    print(f\"   üìÇ {filename_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765c6f0-2c46-4777-8d59-5b14334ee6f5",
   "metadata": {},
   "source": [
    "# Fim - Execucao do Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4958089-b9ac-4282-ae06-efaf20274f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando dados...\n",
      "üîÑ Preparando dados...\n",
      "\n",
      "üöÄ Iniciando Treino de Ensemble com 3 modelos...\n",
      "\n",
      "‚öôÔ∏è Processando: XGBoost\n",
      "   RMSE Valida√ß√£o Interna: $68,588.68\n",
      "   üîÑ Re-treinando em 100% dos dados...\n",
      "\n",
      "‚öôÔ∏è Processando: GradientBoosting\n",
      "   RMSE Valida√ß√£o Interna: $68,792.49\n",
      "   üîÑ Re-treinando em 100% dos dados...\n",
      "\n",
      "‚öôÔ∏è Processando: RandomForest\n",
      "   RMSE Valida√ß√£o Interna: $68,677.83\n",
      "   üîÑ Re-treinando em 100% dos dados...\n",
      "\n",
      "üèÜ Gerando submiss√£o Ensemble...\n",
      "\n",
      "‚úÖ Submiss√£o #6 salva com sucesso!\n",
      "   üìÇ submissoes/submission_6.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Iniciando Otimiza√ß√£o V2 (Target Encoding + Tuned XGB)...\")\n",
    "    \n",
    "    # 1. Carregar\n",
    "    df_treino = pd.read_csv('dados/train.csv', index_col='id')\n",
    "    df_teste = pd.read_csv('dados/test.csv', index_col='id')\n",
    "\n",
    "    # 2. Preparar (J√° devolve o Y em Log e com Target Encoding)\n",
    "    X, y_log, X_test, _ = preparar_dados(df_treino, df_teste)\n",
    "\n",
    "    # 3. Split de Valida√ß√£o\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "    # 4. Modelo XGBoost Tunado\n",
    "    # Adicionei regulariza√ß√£o (reg_alpha, reg_lambda) para o modelo ser mais est√°vel\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=2000,       # Mais √°rvores\n",
    "        learning_rate=0.01,      # Aprender mais devagar (melhor precis√£o)\n",
    "        max_depth=6,\n",
    "        subsample=0.7,           # Usar 70% das linhas por √°rvore (evita overfitting)\n",
    "        colsample_bytree=0.7,    # Usar 70% das colunas por √°rvore\n",
    "        reg_alpha=0.1,           # Regulariza√ß√£o L1\n",
    "        reg_lambda=1.5,          # Regulariza√ß√£o L2\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        early_stopping_rounds=100 # Para se deixar de melhorar\n",
    "    )\n",
    "\n",
    "    print(\"‚öôÔ∏è Treinando modelo com Early Stopping...\")\n",
    "    # O eval_set ajuda o modelo a parar no momento perfeito\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    # 5. Avaliar na Valida√ß√£o\n",
    "    val_preds_log = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(np.expm1(y_val), np.expm1(val_preds_log)))\n",
    "    print(f\"\\n‚úÖ RMSE Valida√ß√£o (D√≥lares Reais): ${rmse:,.2f}\")\n",
    "\n",
    "    # 6. TREINO FINAL (Full Data)\n",
    "    # Como usamos early stopping, o ideal √© retreinar com o n√∫mero √≥timo de √°rvores\n",
    "    best_iter = model.best_iteration\n",
    "    print(f\"üîÑ Retreinando em tudo com {best_iter} √°rvores...\")\n",
    "    \n",
    "    model_final = xgb.XGBRegressor(\n",
    "        n_estimators=best_iter,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.5,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model_final.fit(X, y_log)\n",
    "\n",
    "    # 7. Submiss√£o\n",
    "    preds_test_log = model_final.predict(X_test)\n",
    "    preds_final = np.expm1(preds_test_log) # Inverter Log\n",
    "\n",
    "    df_sub = pd.DataFrame({'id': df_teste.index, 'price': preds_final})\n",
    "    df_sub.to_csv('submission_xgboost_v2.csv', index=False)\n",
    "    print(\"üìÇ Submiss√£o salva: submission_xgboost_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce042e-beca-4e79-ba62-4d872ee9fd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
