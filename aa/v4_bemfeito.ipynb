{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa168ab-4500-4d22-aba9-989df3e3274d",
   "metadata": {},
   "source": [
    "# 1. Import de Bibliotecas e de Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c076668-f3b0-4c9b-849a-b4d4235502b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# Importar modelos\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f614e-fa19-4055-ab8b-e4ed7ffe1037",
   "metadata": {},
   "source": [
    "# 2. FEATURE ENGINEERING - Extracao e Limpeza de Dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7755fa7-15bb-4e13-9d7e-9c5da12eae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engeneering(df):\n",
    "    df_eng = df.copy()\n",
    "\n",
    "    # --- LIMPEZA INICIAL ---\n",
    "    df_eng = df_eng.drop_duplicates()\n",
    "    df_eng['hp'] = df['engine'].str.extract(r'(\\d+\\.?\\d*)HP', expand=False).astype(float)\n",
    "    df_eng['liters'] = df['engine'].str.extract(r'(\\d+\\.?\\d*)L\\s', expand=False).astype(float)\n",
    "\n",
    "    # --- Idade e Uso ---\n",
    "    var_ano_atual = date.today().year\n",
    "    df_eng['car_age'] = var_ano_atual - df_eng['model_year']\n",
    "    df_eng['car_age'] = df_eng['car_age'].replace(0, 1)\n",
    "\n",
    "    # --- Cilindrada ---\n",
    "    df_eng['cylinders'] = df['engine'].str.extract(r'(\\d+)\\s+Cylinder', expand=False)\n",
    "    df_eng['cylinders'] = df_eng['cylinders'].fillna(df['engine'].str.extract(r'V(\\d+)', expand=False))\n",
    "    df_eng['cylinders'] = df_eng['cylinders'].astype(float)\n",
    "\n",
    "    # --- Tecnologias de Motor ---\n",
    "    df_eng['is_turbo'] = df['engine'].str.contains(r'(?i)turbo', na=False).astype(int)\n",
    "    df_eng['turbo_type'] = df['engine'].str.extract(r'(Twin Turbo|Turbo)', expand=False)\n",
    "    df_eng['valve_train'] = df['engine'].str.extract(r'(DOHC|OHV|SOHC)', expand=False) \n",
    "    df_eng['fuel_injection'] = df['engine'].str.extract(r'(PDI|GDI|MPFI)', expand=False)\n",
    "\n",
    "    # Miles per year\n",
    "    df_eng['miles_p_year'] = df_eng['milage'] / df_eng['car_age']\n",
    "\n",
    "    # --- FUEL TYPE ---\n",
    "    def clean_fuel(val):\n",
    "        s = str(val).lower()\n",
    "        if 'hybrid' in s:\n",
    "            return 'Hybrid'\n",
    "        elif 'not supported' in s:\n",
    "            return 'EV'\n",
    "        else:\n",
    "            return val\n",
    "    df_eng['fuel_type'] = df_eng['fuel_type'].apply(clean_fuel)\n",
    "\n",
    "    # --- TRANSMISSION TYPE ---\n",
    "    def clean_transmission(val):\n",
    "        s = str(val).lower()\n",
    "        if 'automatic' in s or 'a/t' in s or 'cvt' in s:\n",
    "            return 'Automatico'\n",
    "        elif 'manual' in s or 'm/t' in s:\n",
    "            return 'Manual'\n",
    "        else:\n",
    "            return 'Outro'\n",
    "    df_eng['transmission_type'] = df_eng['transmission'].apply(clean_transmission)\n",
    "\n",
    "    # --- Cores ---\n",
    "    top_ext_colors = df_eng['ext_col'].value_counts().nlargest(10).index\n",
    "    def simplificar_cor_ext(cor):\n",
    "        return cor if cor in top_ext_colors else 'Other'\n",
    "    df_eng['ext_col_simple'] = df_eng['ext_col'].apply(simplificar_cor_ext)\n",
    "\n",
    "    top_int_colors = df_eng['int_col'].value_counts().nlargest(10).index\n",
    "    def simplificar_cor_int(cor):\n",
    "        return cor if cor in top_int_colors else 'Other'\n",
    "    df_eng['int_col_simple'] = df_eng['int_col'].apply(simplificar_cor_int)\n",
    "\n",
    "    # --- Tratamento de Nulos ---\n",
    "    cols_texto = df_eng.select_dtypes(include=['object']).columns\n",
    "    df_eng[cols_texto] = df_eng[cols_texto].replace('-', 'Unknown').fillna('Unknown')\n",
    "    df_eng['clean_title'] = df_eng['clean_title'].replace('Unknown', 'No')\n",
    "\n",
    "    # --- Acidente ---\n",
    "    def verificar_acidente(valor):\n",
    "        return 0 if 'None' in str(valor) else 1\n",
    "    df_eng['accident_clean'] = df_eng['accident'].apply(verificar_acidente)\n",
    "\n",
    "    # 1. R√°cio de Pot√™ncia por Litro (Efici√™ncia do motor)\n",
    "    # Evitar divis√£o por zero somando um valor √≠nfimo\n",
    "    df_eng['hp_per_liter'] = df_eng['hp'] / (df_eng['liters'] + 0.001)\n",
    "\n",
    "    # 2. R√°cio de Pot√™ncia por Cilindro\n",
    "    df_eng['hp_per_cylinder'] = df_eng['hp'] / (df_eng['cylinders'] + 0.001)\n",
    "\n",
    "    # 3. Log na Quilometragem (Milage)\n",
    "    # A milage tem uma distribui√ß√£o muito \"cauda longa\". O Log ajuda o modelo a ver melhor as diferen√ßas.\n",
    "    df_eng['milage_log'] = np.log1p(df_eng['milage'])\n",
    "\n",
    "    return df_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d4bec-25ba-4934-94ae-5b1b7fde8338",
   "metadata": {},
   "source": [
    "# 3. PREPARA√á√ÉO DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a271f79-8b4a-4e82-973c-5c6148d06d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_dados(df_treino, df_teste):\n",
    "    \"\"\"Prepara dados para modelagem\"\"\"\n",
    "\n",
    "    # Aplicar feature engineering\n",
    "    df_treino_eng = feature_engeneering(df_treino)\n",
    "    df_teste_eng = feature_engeneering(df_teste)\n",
    "\n",
    "    # Separar target\n",
    "    y = df_treino_eng['price']\n",
    "    X = df_treino_eng.drop('price', axis=1)\n",
    "    X_test = df_teste_eng.copy()\n",
    "\n",
    "    # Selecionar features relevantes\n",
    "    features_numericas = ['hp', 'liters', 'car_age', 'cylinders', 'miles_p_year', \n",
    "                          'milage', 'model_year', 'is_turbo']\n",
    "\n",
    "    features_categoricas = ['brand', 'model', 'fuel_type', 'transmission_type', \n",
    "                           'ext_col_simple', 'int_col_simple', 'clean_title', \n",
    "                           'turbo_type', 'valve_train', 'fuel_injection']\n",
    "\n",
    "    # Criar dataset num√©rico\n",
    "    X_num = X[features_numericas].fillna(0)\n",
    "    X_test_num = X_test[features_numericas].fillna(0)\n",
    "\n",
    "    # Encodar vari√°veis categ√≥ricas\n",
    "    X_cat = X[features_categoricas].copy()\n",
    "    X_test_cat = X_test[features_categoricas].copy()\n",
    "\n",
    "    encoders = {}\n",
    "    for col in features_categoricas:\n",
    "        le = LabelEncoder()\n",
    "        # Fit no treino\n",
    "        X_cat[col] = X_cat[col].astype(str)\n",
    "        le.fit(X_cat[col])\n",
    "        X_cat[col] = le.transform(X_cat[col])\n",
    "\n",
    "        # Transform no teste (tratando categorias novas)\n",
    "        X_test_cat[col] = X_test_cat[col].astype(str)\n",
    "        X_test_cat[col] = X_test_cat[col].apply(\n",
    "            lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "        )\n",
    "        encoders[col] = le\n",
    "\n",
    "    # Concatenar features\n",
    "    X_final = pd.concat([X_num, X_cat], axis=1)\n",
    "    X_test_final = pd.concat([X_test_num, X_test_cat], axis=1)\n",
    "\n",
    "    return X_final, y, X_test_final, encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195a7cf-1576-4570-9e78-fb1a6cd84404",
   "metadata": {},
   "source": [
    "# 4. DEFINI√á√ÉO DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08236dd4-e05b-4131-bb66-cbf6ff847536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_modelos():\n",
    "    \"\"\"Retorna dicion√°rio com todos os modelos dispon√≠veis\"\"\"\n",
    "\n",
    "    modelos = {\n",
    "        # Regress√£o Linear\n",
    "        'Linear Regression': LinearRegression(),\n",
    "\n",
    "        # K-Nearest Neighbors\n",
    "        #'KNN': KNeighborsRegressor(),\n",
    "\n",
    "        # √Årvores de Decis√£o\n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "\n",
    "        # XGBoost\n",
    "        'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "\n",
    "        # Support Vector Machines\n",
    "        'SVR Linear': SVR(kernel='linear'),\n",
    "        #'SVR RBF': SVR(kernel='rbf'),\n",
    "\n",
    "        # Redes Neuronais\n",
    "        'MLP Small': MLPRegressor(random_state=42, early_stopping=True),\n",
    "        #'MLP Deep': MLPRegressor(random_state=42, early_stopping=True)\n",
    "    }\n",
    "\n",
    "    return modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102bd35-0181-458e-bd8c-174dcd8a59e9",
   "metadata": {},
   "source": [
    "# 5. GridSearchCV - Grid Search com Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b197afb0-58d0-42b8-8035-f1242807e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. GridSearchCV - Otimiza√ß√£o de Hiperpar√¢metros\n",
    "def obter_params_grid(nome_modelo):\n",
    "    \"\"\"\n",
    "    Retorna o dicion√°rio de hiperpar√¢metros para testar baseado no nome do modelo.\n",
    "    \"\"\"\n",
    "    grids = {\n",
    "        'Linear Regression': {\n",
    "            'fit_intercept': [True, False],\n",
    "            'positive': [False, True]\n",
    "        },\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': [100, 500, 1000],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.7, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.9, 1.0],\n",
    "            'min_child_weight': [1, 3, 5],\n",
    "            'gamma': [0, 0.1, 0.5, 1.0]\n",
    "        },\n",
    "        'KNN': {\n",
    "            'n_neighbors': [3, 5, 7, 11],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'p': [1, 2]\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'max_depth': [5, 10, 15, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'SVR Linear': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'epsilon': [0.01, 0.1, 0.5]\n",
    "        },\n",
    "        'SVR RBF': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "            'epsilon': [0.01, 0.1, 0.5]\n",
    "        },\n",
    "        'MLP Small': {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 25)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001]\n",
    "        },\n",
    "        'MLP Deep': {\n",
    "            'hidden_layer_sizes': [(100, 50), (100, 50, 25), (50, 50, 50)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001]\n",
    "        }\n",
    "    }\n",
    "    return grids.get(nome_modelo, {})\n",
    "\n",
    "def executar_grid_search(modelo, nome_modelo, X, y, usar_scaled=False):\n",
    "    \"\"\"\n",
    "    Executa o GridSearch para encontrar os melhores hiperpar√¢metros.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Iniciando GridSearch para: {nome_modelo}\")\n",
    "\n",
    "    # 1. Obter a grelha de par√¢metros\n",
    "    param_grid = obter_params_grid(nome_modelo)\n",
    "\n",
    "    if not param_grid:\n",
    "        print(f\"‚ö†Ô∏è Nenhuma grid definida para {nome_modelo}. Retornando modelo base.\")\n",
    "        return modelo\n",
    "\n",
    "    # 2. Configurar o GridSearch\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=modelo,\n",
    "        param_grid=param_grid,\n",
    "        cv=3, \n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        verbose=1,\n",
    "        n_jobs=-1  # Usa todos os processadores\n",
    "    )\n",
    "\n",
    "    # 3. Treinar\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # 4. Resultados\n",
    "    print(f\"‚úÖ Melhores Par√¢metros: {grid_search.best_params_}\")\n",
    "    print(f\"   Melhor RMSE (CV): {-grid_search.best_score_:,.2f}\")\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092de71f-bf19-47ce-bdb7-3571e098690c",
   "metadata": {},
   "source": [
    "# 6. AVALIA√á√ÉO DE MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e5ca01-d83a-4a23-83d3-f467ea95343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_modelo(modelo, X_train, y_train, X_val, y_val, nome_modelo):\n",
    "    \"\"\"Treina e avalia um modelo\"\"\"\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Treinando: {nome_modelo}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Treinar\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    # Predi√ß√µes\n",
    "    y_train_pred = modelo.predict(X_train)\n",
    "    y_val_pred = modelo.predict(X_val)\n",
    "\n",
    "    # M√©tricas de treino\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "\n",
    "    # M√©tricas de valida√ß√£o\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "    # Exibir resultados\n",
    "    print(f\"\\nüìä M√âTRICAS DE TREINO:\")\n",
    "    print(f\"  RMSE: {train_rmse:,.2f}\")\n",
    "\n",
    "    print(f\"\\nüìä M√âTRICAS DE VALIDA√á√ÉO:\")\n",
    "    print(f\"  RMSE: {val_rmse:,.2f}\")\n",
    "\n",
    "    return {\n",
    "        'modelo': nome_modelo,\n",
    "        'train_rmse': train_rmse,\n",
    "        'val_rmse': val_rmse,\n",
    "        'modelo_treinado': modelo\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ed31d-3e69-4843-a906-ab208a792bb3",
   "metadata": {},
   "source": [
    "# 7. TREINAR MODELOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94ac6f85-9dad-4fcd-8f5e-cbf989845ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelos(df_treino, df_teste, modelos_selecionados=None):\n",
    "    \"\"\"\n",
    "    Treina e compara m√∫ltiplos modelos\n",
    "\n",
    "    Args:\n",
    "        df_treino: DataFrame de treino\n",
    "        df_teste: DataFrame de teste\n",
    "        modelos_selecionados: Lista com nomes dos modelos a treinar (None = todos)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîÑ Preparando dados...\")\n",
    "    X, y, X_test, encoders = preparar_dados(df_treino, df_teste)\n",
    "\n",
    "    # Aplicar log-transform ao target (importante para distribui√ß√£o dos pre√ßos)\n",
    "    y_log = np.log1p(y)\n",
    "\n",
    "    # Split treino/valida√ß√£o\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Normalizar dados (importante para KNN, SVM e MLP)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"‚úÖ Dados preparados:\")\n",
    "    print(f\"  - Treino: {X_train.shape[0]} amostras\")\n",
    "    print(f\"  - Valida√ß√£o: {X_val.shape[0]} amostras\")\n",
    "    print(f\"  - Features: {X_train.shape[1]}\")\n",
    "\n",
    "    # Obter modelos\n",
    "    todos_modelos = obter_modelos()\n",
    "\n",
    "    # Filtrar modelos se especificado\n",
    "    if modelos_selecionados:\n",
    "        modelos = {k: v for k, v in todos_modelos.items() if k in modelos_selecionados}\n",
    "    else:\n",
    "        modelos = todos_modelos\n",
    "\n",
    "    # Treinar modelos\n",
    "    resultados = []\n",
    "    modelos_treinados = {}\n",
    "\n",
    "    for nome, modelo in modelos.items():\n",
    "        # Decidir se usar dados normalizados\n",
    "        usar_scaled = nome in ['KNN', 'SVR Linear', 'SVR RBF', 'MLP Small', 'MLP Deep']\n",
    "\n",
    "        if usar_scaled:\n",
    "            # Executar GridSearch para otimizar hiperpar√¢metros\n",
    "            modelo_otimizado = executar_grid_search(modelo, nome, X_train_scaled, y_train, usar_scaled=True)\n",
    "            res = avaliar_modelo(modelo_otimizado, X_train_scaled, y_train, \n",
    "                                X_val_scaled, y_val, nome)\n",
    "        else:\n",
    "            # Executar GridSearch para otimizar hiperpar√¢metros\n",
    "            modelo_otimizado = executar_grid_search(modelo, nome, X_train, y_train, usar_scaled=False)\n",
    "            res = avaliar_modelo(modelo_otimizado, X_train, y_train, \n",
    "                                X_val, y_val, nome)\n",
    "\n",
    "        resultados.append(res)\n",
    "        modelos_treinados[nome] = {\n",
    "            'modelo': res['modelo_treinado'],\n",
    "            'usar_scaled': usar_scaled\n",
    "        }\n",
    "\n",
    "    # Compara√ß√£o final\n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"üìà COMPARA√á√ÉO DE MODELOS (ordenados por RMSE de valida√ß√£o)\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    df_resultados = df_resultados.sort_values('val_rmse', ascending=True)\n",
    "\n",
    "    print(df_resultados[['modelo', 'train_rmse', 'val_rmse']].to_string(index=False))\n",
    "\n",
    "    # Melhor modelo\n",
    "    melhor = df_resultados.iloc[0]\n",
    "    print(f\"\\n\\nüèÜ MELHOR MODELO: {melhor['modelo']}\")\n",
    "    print(f\"  RMSE Valida√ß√£o: ${melhor['val_rmse']:,.2f}\")\n",
    "\n",
    "    return df_resultados, modelos_treinados, scaler, (X_test, X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2617259-8839-42b6-90ae-342a9f94ee44",
   "metadata": {},
   "source": [
    "# 8. Guardar Hiperparametros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44766b71-4b34-414b-9a42-57479e8fa907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_submissao_log(df_sub, modelo_treinado, nome_modelo, metricas):\n",
    "    \"\"\"\n",
    "    Salva CSV e JSON incrementando o ID com base no maior n√∫mero encontrado.\n",
    "    \"\"\"\n",
    "    pasta = 'submissoes'\n",
    "    os.makedirs(pasta, exist_ok=True)\n",
    "\n",
    "    # 1. Listar arquivos e encontrar o maior ID existente\n",
    "    arquivos = os.listdir(pasta)\n",
    "    ids_existentes = []\n",
    "\n",
    "    for f in arquivos:\n",
    "        # Verifica se o arquivo segue o padr√£o 'submission_X.csv'\n",
    "        if f.startswith('submission_') and f.endswith('.csv'):\n",
    "            try:\n",
    "                # Extrai apenas o n√∫mero do nome do arquivo\n",
    "                # Ex: 'submission_12.csv' -> '12'\n",
    "                numero_str = f.replace('submission_', '').replace('.csv', '')\n",
    "                ids_existentes.append(int(numero_str))\n",
    "            except ValueError:\n",
    "                continue # Pula arquivos que n√£o tenham n√∫mero v√°lido\n",
    "\n",
    "    # Se a lista estiver vazia, come√ßa do 1. Se n√£o, pega o maior + 1\n",
    "    if not ids_existentes:\n",
    "        next_id = 1\n",
    "    else:\n",
    "        next_id = max(ids_existentes) + 1\n",
    "\n",
    "    # 2. Definir nomes dos arquivos\n",
    "    filename_csv = f\"{pasta}/submission_{next_id}.csv\"\n",
    "    filename_json = f\"{pasta}/submission_{next_id}_params.json\"\n",
    "\n",
    "    # 3. Salvar CSV\n",
    "    df_sub.to_csv(filename_csv, index=False)\n",
    "\n",
    "    # 4. Extrair Hiperpar√¢metros\n",
    "    try:\n",
    "        params = modelo_treinado.get_params()\n",
    "    except:\n",
    "        params = {\"info\": \"N√£o foi poss√≠vel extrair params\"}\n",
    "\n",
    "    # 5. Metadata\n",
    "    metadata = {\n",
    "        \"id\": next_id,\n",
    "        \"modelo\": nome_modelo,\n",
    "        \"performance_validacao\": metricas,\n",
    "        \"hiperparametros\": params\n",
    "    }\n",
    "\n",
    "    # 6. Salvar JSON\n",
    "    with open(filename_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=4, default=str)\n",
    "\n",
    "    print(f\"\\n‚úÖ Submiss√£o #{next_id} salva com sucesso!\")\n",
    "    print(f\"   üìÇ {filename_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765c6f0-2c46-4777-8d59-5b14334ee6f5",
   "metadata": {},
   "source": [
    "# Fim - Execucao do Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4958089-b9ac-4282-ae06-efaf20274f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Carregando dados...\n",
      "\n",
      "üöÄ INICIANDO TREINO DE M√öLTIPLOS MODELOS...\n",
      "\n",
      "üîÑ Preparando dados...\n",
      "‚úÖ Dados preparados:\n",
      "  - Treino: 150826 amostras\n",
      "  - Valida√ß√£o: 37707 amostras\n",
      "  - Features: 18\n",
      "\n",
      "üîç Iniciando GridSearch para: Linear Regression\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "‚úÖ Melhores Par√¢metros: {'fit_intercept': True, 'positive': False}\n",
      "   Melhor RMSE (CV): 0.54\n",
      "\n",
      "============================================================\n",
      "Treinando: Linear Regression\n",
      "============================================================\n",
      "\n",
      "üìä M√âTRICAS DE TREINO:\n",
      "  RMSE: 0.54\n",
      "\n",
      "üìä M√âTRICAS DE VALIDA√á√ÉO:\n",
      "  RMSE: 0.54\n",
      "\n",
      "üîç Iniciando GridSearch para: Decision Tree\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "‚úÖ Melhores Par√¢metros: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10}\n",
      "   Melhor RMSE (CV): 0.52\n",
      "\n",
      "============================================================\n",
      "Treinando: Decision Tree\n",
      "============================================================\n",
      "\n",
      "üìä M√âTRICAS DE TREINO:\n",
      "  RMSE: 0.49\n",
      "\n",
      "üìä M√âTRICAS DE VALIDA√á√ÉO:\n",
      "  RMSE: 0.51\n",
      "\n",
      "üîç Iniciando GridSearch para: XGBoost\n",
      "Fitting 3 folds for each of 2916 candidates, totalling 8748 fits\n",
      "‚úÖ Melhores Par√¢metros: {'colsample_bytree': 0.7, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "   Melhor RMSE (CV): 0.49\n",
      "\n",
      "============================================================\n",
      "Treinando: XGBoost\n",
      "============================================================\n",
      "\n",
      "üìä M√âTRICAS DE TREINO:\n",
      "  RMSE: 0.47\n",
      "\n",
      "üìä M√âTRICAS DE VALIDA√á√ÉO:\n",
      "  RMSE: 0.49\n",
      "\n",
      "üîç Iniciando GridSearch para: SVR Linear\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Carregar dados\n",
    "    print(\"üìÇ Carregando dados...\")\n",
    "    df_treino = pd.read_csv('dados/train.csv', index_col='id')\n",
    "    df_teste = pd.read_csv('dados/test.csv', index_col='id')\n",
    "\n",
    "    # 2. Treinar e comparar m√∫ltiplos modelos\n",
    "    print(\"\\nüöÄ INICIANDO TREINO DE M√öLTIPLOS MODELOS...\\n\")\n",
    "    df_resultados, modelos_treinados, scaler, (X_test, X_test_scaled) = treinar_modelos(df_treino, df_teste)\n",
    "\n",
    "    # 3. Selecionar melhor modelo\n",
    "    melhor_resultado = df_resultados.iloc[0]\n",
    "    melhor_modelo_nome = melhor_resultado['modelo']\n",
    "    melhor_modelo = modelos_treinados[melhor_modelo_nome]['modelo']\n",
    "    usar_scaled = modelos_treinados[melhor_modelo_nome]['usar_scaled']\n",
    "\n",
    "    # 4. Fazer previs√µes no teste\n",
    "    if usar_scaled:\n",
    "        X_test_usar = X_test_scaled\n",
    "    else:\n",
    "        X_test_usar = X_test\n",
    "\n",
    "    pred_log = melhor_modelo.predict(X_test_usar)\n",
    "    pred_reais = np.expm1(pred_log)\n",
    "\n",
    "    # 5. Submiss√£o\n",
    "    print(f\"\\nüèÜ Gerando submiss√£o com melhor modelo: {melhor_modelo_nome}...\")\n",
    "    df_submissao = pd.DataFrame({\n",
    "        'id': df_teste.index,\n",
    "        'price': pred_reais\n",
    "    })\n",
    "\n",
    "    salvar_submissao_log(df_submissao, melhor_modelo, melhor_modelo_nome, melhor_resultado.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce042e-beca-4e79-ba62-4d872ee9fd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
